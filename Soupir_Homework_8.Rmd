---
title: "Homework 8"
author: "Alex Soupir"
date: "November 7, 2019"
output:
  pdf_document:
    keep_md: true
    df_print: paged
---

*Packages*: HSAUR3, quantreg, TH.data, gamlss.data, lattice, GGplot2

*Collaborators*: 

**Chapter 12**

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig_height=10,fig_width=7,cache = F)
```

Answer all questions specified on the problem and include a discussion on how your results answered/addressed the question.

Submit your \textbf{.rmd} file with the knitted \textbf{PDF} (or knitted Word Document saved as a PDF). If you are having trouble with .rmd, let us know and we will help you, but both the .rmd and the PDF are required.

This file can be used as a skeleton document for your code/write up. Please follow the instructions found under Content for Formatting and Guidelines. No code should be in your PDF write-up unless stated otherwise.

For any question asking for plots/graphs, please do as the question asks as well as do the same but using the respective commands in the GGPLOT2 library. (So if the question asks for one plot, your results should have two plots. One produced using the given R-function and one produced from the GGPLOT2 equivalent). This doesn't apply to questions that don't specifically ask for a plot, however I still would encourage you to produce both.

You do not need to include the above statements.

Please do the following problems from the text book R Handbook and stated.

<!--
#don't overplot distributions
xyplot(head ~ age | cut, data = db, xlab = "Age (years)",
       ylab = "Head circumference (cm)",
       scales = list(x = list(relation = "free")),
       layout = c(2, 1), pch = 19,
       col = rgb(.1, .1, .1, .1))


-->

1. Consider the {\textbf{clouds}} data from the {\textbf{HSAUR3}} package
   
    a) Review the linear model fitted to this data in Chapter 6 of the text book and report the model and findings. 
    
    ```{r, echo=FALSE}
    library(HSAUR3)
    library(ggplot2)
    set.seed(333)
    
    data(clouds)
    
    clouds_formula = rainfall ~ seeding + seeding:(sne + cloudcover + prewetness + echomotion) + time
    #Xstar = model.matrix(clouds_formula, data = clouds)
    
    #attr(Xstar, "contrasts")
    
    clouds_lm = lm(clouds_formula, data = clouds)
    #class(clouds_lm)
    
    message("Summary of Chapter 6 Model")
    summary(clouds_lm)
    betastar = coef(clouds_lm)
    clouds_resid = residuals(clouds_lm)
    clouds_fitted = fitted(clouds_lm)
    psymb = as.numeric(clouds$seeding)
    plot(rainfall ~ sne, data = clouds, pch = psymb, xlab = "S-Ne criterion")
    abline(lm(rainfall~sne, data=clouds, subset= seeding == 'no'))
    abline(lm(rainfall~sne, data = clouds, subset = seeding == "yes"), lty = 2)
    legend("topright", legend = c("No seeding", "Seeding"), pch = 1:2, lty = 1:2, bty = "n")
    ggplot(clouds, aes(x=sne, y=rainfall, shape=seeding, color=seeding))+geom_point(size=2)+geom_smooth(method=lm,se=FALSE, fullrange=TRUE,size=.5)
    ```
    
    **Above is the model summary as well as the plot of the relationship between the suitability criterion and rainfall with and without seeding. The summary shows that a model with seeding is better able to predict the rainfall than a model with no cloud seeding. Seeding clouds and suitability criterion also shows significance to the model meaning that the effect of seeding clouds is not standalone but also depends on the suitability criterion for determining the rainfall.**
      
    b) Fit a median regression model. 
    
    ```{r, echo=FALSE}
    library(quantreg)
    set.seed(333)
    rq_seed= rq(clouds_formula, data = clouds, tau = 0.5)
    summary(rq_seed, se = "rank")$coef
    
    plot(rainfall~sne, data=clouds, pch=psymb, xlab="S-Ne Criterion")
    abline(rq(rainfall~sne,data=clouds,tau=0.5,seeding== "no"))
    abline(rq(rainfall~sne,data=clouds,tau=0.5,seeding=="yes"),lty=2)
    legend("topright",legend=c("No seeding", "Seeding"), pch=1:2, lty=1:2, bty="n")
    #summary(rq_seed)
    
    ggplot(clouds, aes(x=sne, y=rainfall, shape=seeding, color=seeding))+geom_point(size=2)+geom_smooth(method=rq,se=FALSE, fullrange=TRUE,size=.5)
    
    ```
    
    **A median regression model was fit using *tau = 0.5* on the same formula from the book that was used in *Part A*. The plot was created the same was as *Part A* as well but using *rq* instead of *lm* and adding *tau*.**

    c) Compare the two results. 
    
    ```{r, echo=FALSE}
    set.seed(333)
    mod1a_no = lm(rainfall~sne, data=clouds, subset= seeding == 'no')
    mod1a_yes = lm(rainfall~sne, data = clouds, subset = seeding == "yes")
    mod1b_no = rq(rainfall~sne,data=clouds,tau=0.5,seeding== "no")
    mod1b_yes = rq(rainfall~sne,data=clouds,tau=0.5,seeding=="yes")
    
    preds1a_no = predict(mod1a_no,type="response")
    preds1a_yes = predict(mod1a_yes,type="response")
    preds1b_no = predict(mod1b_no,type="response")
    preds1b_yes = predict(mod1b_yes,type="response")
    
    obs1_no = subset(clouds, seeding == "no")$rainfall
    obs1_yes = subset(clouds, seeding == "yes")$rainfall
    ```
    
    ```{r, eval=FALSE, echo=FALSE}
    
    cat("\nMean squared error of linear model on not seeding data: \n")
    mean((preds1a_no - obs1_no)^2)
    cat(c("AIC: ", AIC(mod1a_no), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds1a_no-obs1_no)), "\n"))
    cat("\nMean squared error of linear model on seeding data: \n")
    mean((preds1a_yes - obs1_yes)^2)
    cat(c("AIC: ", AIC(mod1a_yes), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds1a_yes-obs1_yes)), "\n"))
    cat("\nMean squared error of median model on not seeding data: \n")
    mean((preds1b_no - obs1_no)^2)
    cat(c("AIC: ", AIC(mod1b_no), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds1b_no-obs1_no)), "\n"))
    cat("\nMean squared error of median model on seeding data: \n")
    mean((preds1b_yes - obs1_yes)^2)
    cat(c("AIC: ", AIC(mod1b_yes), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds1b_yes-obs1_yes)), "\n"))
    
    preds_6l = predict(clouds_lm, type="response")
    preds_6m = predict(rq_seed, type="response")
    obs_6 = clouds$rainfall
    
    cat("\nMean squared error of Chapter 6 linear model: \n")
    mean((preds_6l - obs_6)^2)
    cat(c("AIC: ", AIC(clouds_lm), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds_6l-obs_6)), "\n"))
    cat("\nAIC of Chapter 6 median regression: \n")
    mean((preds_6m - obs_6)^2)
    cat(c("AIC: ", AIC(rq_seed), "\n"))
    cat(c("mean absolute error: ", mean(abs(preds_6m-obs_6)), "\n"))
    ```
    
    ```{r, echo=FALSE}
    preds_6l = predict(clouds_lm, type="response")
    preds_6m = predict(rq_seed, type="response")
    obs_6 = clouds$rainfall
    
    lin_med = data.frame("Chapt 6 Linear" = c(mean((preds_6l - obs_6)^2), 
                                              AIC(clouds_lm),
                                              mean(abs(preds_6l-obs_6))),
                         "Chapt 6 Median" = c(mean((preds_6m - obs_6)^2),
                                              AIC(rq_seed),
                                              mean(abs(preds_6m-obs_6))))
    row.names(lin_med) = c("MSE","AIC","MAE")
    print(lin_med)
    
    cat("\nCoefficients of linear and median models using clouds_formula from book: \n")
    coeffs = data.frame(coefficients(clouds_lm))
    coeffs = cbind(coeffs, data.frame(rq_seed$coefficients))
    colnames(coeffs) = c("Linear","Median")
    print(coeffs)
    ```
    <!--
    
    **Comparing the plots produced in parts *A* and *B*, the largest difference in the change in the *No seeding* line where it went from having a negative slope when using the linear model to having a positive slope using the median regression. This could be due to the extreme point at about (1.7, 13) doesn't have as much 'pull' in the model anymore. The slope of the seeding model from linear to median increased slightly. The linear model produced a lower MSE for both the data created with cloud seeding and without cloud seeding. Additionally, the coefficients for both models were placed into a table similarly to in lecture. The seeding coefficient for when clouds are seeded is rather different between the linear model and the median model.**
    **The short models that were created for plots showed the linear models producing lower mean squared error values, however looking at the AIC the median regression models are better fitters of the input data even though prediction isn't as great. This follows that of the model created in Chapter 6 that was fit for linear and median regression; the median regression produced a lower AIC value than did the linear regression model.**
    **Mean absolute error is like the AIC with the median regression models producing lower error, most likely due to less impact by the outliers of the data. **
    -->
    
    **The linear model of the equation from chapter 6 shows a lower mean squared error than the median model. However, the median model does produce a lower mean absolute error and lower AIC than the linear model. This could be because the median model isn't taking the outliers into account as much as a mean weighted calculation would, but this could produce a higher error if the model shifts too far from the outliers to fit other values better.**
    
2. Reanalyze the {\textbf{bodyfat}} data from the {\textbf{TH.data}} package. 

    a) Compare the regression tree approach from chapter 9 of the textbook to median regression and summarize the different findings.
    
    ```{r, echo=FALSE}
    library(TH.data)
    set.seed(333)
    
    data(bodyfat)
    
    bdft_equation = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
    
    library(rpart)
    bdft_rpart = rpart(bdft_equation, data = bodyfat, control = rpart.control(minsplit=10))
    
    library(partykit)
    plot(as.party(bdft_rpart), tp_args = list(id = FALSE))
    
    opt <- which.min(bdft_rpart$cptable[,"xerror"])
    cp <- bdft_rpart$cptable[opt, "CP"]
    bdft_prune <- prune(bdft_rpart, cp = cp)
    plot(as.party(bdft_prune), tp_args = list(id = FALSE))
    
    #cat("MSE of tree using Chapter 9 equation without pruning: \n")
    bdft_pred = predict(bdft_rpart, newdata = bodyfat)
    bdft_mse = mean((bodyfat$DEXfat - bdft_pred)^2)
    bdft_mae = mean(abs(bodyfat$DEXfat-bdft_pred))
    
    #cat("MSE of tree using Chapter 9 equation with pruning: \n")
    bdft_pred2 = predict(bdft_prune, newdata = bodyfat)
    bdft_mse2 = mean((bodyfat$DEXfat - bdft_pred2)^2)
    bdft_mae2 = mean(abs(bodyfat$DEXfat-bdft_pred2))
    
    #cat("MSE of median regression of original equation: \n")
    bdft_med1= rq(bdft_equation, data = bodyfat, tau = 0.5)
    bdft_medpred1 = predict(bdft_med1, type="response")
    bdft_medmse1 = mean((bodyfat$DEXfat - bdft_medpred1)^2)
    bdft_medmae1 = mean(abs(bodyfat$DEXfat-bdft_medpred1))
    
    tre_med = data.frame("Unpruned.Tree" = c(bdft_mse, bdft_mae),
                         "Pruned.Tree" = c(bdft_mse2, bdft_mae2),
                         "Median.Reg" = c(bdft_medmse1, bdft_medmae1))
    row.names(tre_med) = c("MSE","MAE")
    print(tre_med)
    ```
    
    **Comparing the regression tree's, both unpruned and pruned, shows that the unpruned tree has lower mean squared error in the predicted DEXfat. A median regression was also created and the mean squared error was calculated. The mean squared error of the median regression was greater than the pruned tree. The mean absolute error of the 3 models shows that the unpruned tree has the least amount of error followed by the median regression. The pruned tree's error is very close to the median regression error but higher.**
    
    b) Choose one independent variable. For the relationship between this variable and DEXfat, create linear regression quantile models for the 25%, 50% and 75% quantiles. Plot DEXfat vs that independent variable and plot the lines from the models on the graph. 
    
    ```{r, echo=FALSE}
    set.seed(333)
    waist = rq(DEXfat ~ waistcirc, data = bodyfat, tau=c(0.25, 0.5, 0.75))
    summary(waist)
    
    plot(DEXfat ~ waistcirc, data = bodyfat, xlab = "Waist Circumference")
    abline(rq(DEXfat ~ waistcirc, data = bodyfat, tau = 0.25),lty = 1)
    abline(rq(DEXfat ~ waistcirc, data = bodyfat, tau = 0.5), lty = 2)
    abline(rq(DEXfat ~ waistcirc, data = bodyfat, tau = 0.75),lty = 3)
    legend("topleft", legend = c("25%", "50%", "75%"), lty = 1:3, bty = "n")
    
    med_reg2b = data.frame(t(coef(waist)))
    colnames(med_reg2b) = c("intercept","slope")
    qplot(x=waistcirc, y=DEXfat, data=bodyfat) + geom_abline(aes(intercept=intercept, slope=slope, linetype=row.names(med_reg2b)), data=med_reg2b)
    ```
    
3. Consider {\textbf{db}} data from the lecture notes (package {\textbf{gamlss.data}}). Refit the additive quantile regression models presented ({\textbf{rqssmod}}) with varying values of $\lambda$ (lambda) in {\textbf{qss}}. How do the estimated quantile curves change?

```{r, echo=FALSE, fig.height=4}
library(gamlss.data)
library(lattice)
set.seed(333)

data(db)
db2 <- db
tau <- c(.03, .15, .5, .85, .97)

#lambda 1
rqssmod <- vector(mode = "list", length = length(tau))
db2$lage <- with(db2, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] <- rqss(head ~ qss(lage, lambda = 1),
                       data = db2, tau = tau[i])

gage <- seq(from = min(db2$age), to = max(db2$age), length = 100)
p <- sapply(1:length(tau), function(i) { predict(rqssmod[[i]],
                                                 newdata = data.frame(lage = gage^(1/3)))
})

## Playing with the whole db data 
pfun <- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db2$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
  #panel.text(rep(min(db2$age), length(tau)),
  #p[1,], label = tau, cex = 0.9)
}
xyplot(head ~ age, data = db2, main = "Lambda = 1", xlab = "Age (years)",
       ylab = "Head circumference (cm)", pch = 16,
       scales = list(x = list(relation = "free")),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)
panel = pfun
ggplot(data = db, aes(y=head ,x= age))+geom_point(alpha=0.1)+geom_quantile(quantiles=tau,method = "rqss", lambda = 1)+ggtitle("Lambda=1")+xlab("Age (years)")+ylab("Head circumference (cm)")

#lambda 10
rqssmod <- vector(mode = "list", length = length(tau))
db2$lage <- with(db2, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] <- rqss(head ~ qss(lage, lambda = 10),
                       data = db2, tau = tau[i])

gage <- seq(from = min(db2$age), to = max(db2$age), length = 100)
p <- sapply(1:length(tau), function(i) { predict(rqssmod[[i]],
                                                 newdata = data.frame(lage = gage^(1/3)))
})

## Playing with the whole db data 
pfun <- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db2$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
  #panel.text(rep(min(db2$age), length(tau)),
  #p[1,], label = tau, cex = 0.9)
}
xyplot(head ~ age, data = db2, main = "Lambda = 10", xlab = "Age (years)",
       ylab = "Head circumference (cm)", pch = 16,
       scales = list(x = list(relation = "free")),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)
panel = pfun
ggplot(data = db, aes(y=head ,x= age))+geom_point(alpha=0.1)+geom_quantile(quantiles=tau,method = "rqss", lambda = 10)+ggtitle("Lambda=10")+xlab("Age (years)")+ylab("Head circumference (cm)")

#lambda 20
rqssmod <- vector(mode = "list", length = length(tau))
db2$lage <- with(db2, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] <- rqss(head ~ qss(lage, lambda = 20),
                       data = db2, tau = tau[i])

gage <- seq(from = min(db2$age), to = max(db2$age), length = 100)
p <- sapply(1:length(tau), function(i) { predict(rqssmod[[i]],
                                                 newdata = data.frame(lage = gage^(1/3)))
})

## Playing with the whole db data 
pfun <- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db2$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
  #panel.text(rep(min(db2$age), length(tau)),
  #p[1,], label = tau, cex = 0.9)
}
xyplot(head ~ age, data = db2, main = "Lambda = 20", xlab = "Age (years)",
       ylab = "Head circumference (cm)", pch = 16,
       scales = list(x = list(relation = "free")),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)
panel = pfun
ggplot(data = db, aes(y=head ,x= age))+geom_point(alpha=0.1)+geom_quantile(quantiles=tau,method = "rqss", lambda = 20)+ggtitle("Lambda=20")+xlab("Age (years)")+ylab("Head circumference (cm)")

#lambda 50
rqssmod <- vector(mode = "list", length = length(tau))
db2$lage <- with(db2, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] <- rqss(head ~ qss(lage, lambda = 50),
                       data = db2, tau = tau[i])

gage <- seq(from = min(db2$age), to = max(db2$age), length = 100)
p <- sapply(1:length(tau), function(i) { predict(rqssmod[[i]],
                                                 newdata = data.frame(lage = gage^(1/3)))
})

## Playing with the whole db data 
pfun <- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db2$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
  #panel.text(rep(min(db2$age), length(tau)),
  #p[1,], label = tau, cex = 0.9)
}
xyplot(head ~ age, data = db2, main = "Lambda = 50", xlab = "Age (years)",
       ylab = "Head circumference (cm)", pch = 16,
       scales = list(x = list(relation = "free")),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)
panel = pfun
ggplot(data = db, aes(y=head ,x= age))+geom_point(alpha=0.1)+geom_quantile(quantiles=tau,method = "rqss", lambda = 50)+ggtitle("Lambda=50")+xlab("Age (years)")+ylab("Head circumference (cm)")

#lambda 100
rqssmod <- vector(mode = "list", length = length(tau))
db2$lage <- with(db2, age^(1/3))
for (i in 1:length(tau))
  rqssmod[[i]] <- rqss(head ~ qss(lage, lambda = 100),
                       data = db2, tau = tau[i])

gage <- seq(from = min(db2$age), to = max(db2$age), length = 100)
p <- sapply(1:length(tau), function(i) { predict(rqssmod[[i]],
                                                 newdata = data.frame(lage = gage^(1/3)))
})

## Playing with the whole db data 
pfun <- function(x, y, ...) {
  panel.xyplot(x = x, y = y, ...)
  apply(p, 2, function(x) panel.lines(gage, x))
  panel.text(rep(max(db2$age), length(tau)),
             p[nrow(p),], label = tau, cex = 0.9)
  #panel.text(rep(min(db2$age), length(tau)),
  #p[1,], label = tau, cex = 0.9)
}
xyplot(head ~ age, data = db2, main = "Lambda = 100", xlab = "Age (years)",
       ylab = "Head circumference (cm)", pch = 16,
       scales = list(x = list(relation = "free")),
       layout = c(1, 1), col = rgb(.1, .1, .1, .1),
       panel = pfun)
panel = pfun
ggplot(data = db, aes(y=head ,x= age))+geom_point(alpha=0.1)+geom_quantile(quantiles=tau,method = "rqss", lambda = 100)+ggtitle("Lambda=100")+xlab("Age (years)")+ylab("Head circumference (cm)")
```

**An increase in lambda, which is the penalty for the smoothness as said in lecture, smooths the quantile curves out more and more as it increases to 100 from 1. The 0.03 line also starts getting further away from the body of the data between 0 and 5 years of age as the function tries to smooth it out. A lambda of 1 which was generated in lecture keeps the lines tightly around the data where there are sharp curves between 0 and 5, and also has more 'wiggling' from about 5 to the end. An increase in lambda causes smoother curves.**
  
4. Read the paper by Koenker and Hallock (2001), posted on D2L. Write a one page summary of the paper. This should include but not be limited to introduction, motivation, case study considered and findings.

****

**Introduction**

Using quantiles while displaying the data can provide the viewer with a much better understanding the data as a whole rather than only using the mean. The use of quantiles (percentiles or fractiles) allows the sampling groups to be split into groups based on the sample values. For example, a percentile of 0.25 will split the data into 2 groups, one of 25% and another with 75% of the data. Figures like boxplots and scatter plots with growth curves offer more transparent observations of data dispersion than do that of bar plots. There are other plots that utilize quantiles for displaying data which were talked about throughout the article like Quantile Engel Curves and quantile regressions.

**Motivation**

Quanltile regressions are more appropriate for data with dispersion and outliers because outlying data may pull averages away form the 50th percentile. Quantiles via optimizaiton shows how the minimizaiton of the sum of absolute residuals is better in some cases than is the minimizing a sum of squared residuals for means. Koenker and Hallock show how the least squares estimates can change the regression line over a median regression by only a few data points that may potentially be outliers and density of low household income and high food expenditures (mean line falls below a large portion of these points).

**Case Study**

The case study that was considered in this article was about using quantile regression and dertminants of infant birthweights conducted by Detailed Natality Data in 1997, which had investigated by Abrevaya in 2001. This was because most of the birthweight analyses are done using the least squares approach and not the least absolute residual values which, as we saw in the previous section, causes issues when working with data that have a high concentration towards one side. Along with the weight of the babies (g), several other metrics were recorded like race, mothers education, prenatal medical care, how much the mother smoked during pregnancy, mothers weight gain during pregnancy, and the age of the baby. If there were any data point metrics missing that baby was removed from the analysis.

**Findings**

The researchers found that boys are about 100 grams heavier than girls on average, but looking at the quantiles boys are much heavier than girls closer to the upper end of the distribution (about 130 - 140 grams at the 0.95 quantile). They also mention that the difference between black and white mothers is also relatively high. Looking at the graph the average difference shows that babies of black mothers are about 200 grams less than babies from white mothers. However, the babies on the lower side of the distribution close to 350 grams lighter than babies from white mothers while those babies from black mothers on the upper side of the distribution are about 160 grams lighter than babies from white mothers. They also note that there is little difference between sum of squared residuals and sum of absolute residuals for different education levels, and with the exception of education, all of the other plots show that the quantile regression falls outside of the 90% confidence interval at some point. 

****

*Resources Used*:

+ rdocumentation.org
+ theanalysisfactor.com
+ r-bloggers.com
+ Quantile Regression - Roger Koenker and Kevin Hallock




























