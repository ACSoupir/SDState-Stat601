---
title: "Homework 2"
author: "Alex Soupir"
date: "September 05, 2019"
output:
  pdf_document:
    keep_md: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig_height=10,fig_width=7,cache = F)
```

*Packages*: HSAUR3, GGplot, gamair, MASS, ISLR

*Collaborators*:

Please do the following problems from the text book R Handbook and stated.

1. Collett (2003) argues that two outliers need to be removed from the \textbf{plasma} data. Try to identify those two unusual observations by means of a scatterplot. (7.2 on Handbook)
+ Statement: Find two outliers that are argued to be removed from the **plasma** df.

```{r, echo=FALSE}
#importing librar
library(HSAUR3)

plasma.df = plasma
attach(plasma.df)

#create plots
plot(fibrinogen, globulin, col=c('red', 'blue')[ESR])
legend(x='topright', legend=levels(plasma.df$ESR), col=c('red', 'blue'), pch=1)

library(ggplot2)
ggplot(plasma.df, aes(x=fibrinogen, y=globulin, col=ESR)) + geom_point(shape = 1)

detach(plasma.df)
```
+ **Discussion: The 2 observations that I think are the outliers by looking at the scatter plots from both *ggplot* and base R includes the observation with which fibrinogen is greater than 5 and fibrogen close to 4. These 2 points are located relatively by themselves whereas the observations that have globulin close to (and greater than) 45 have several observations at that level.**
    + **It's difficult to determine the outlier just by viewwing the data without a formal test that states emperically the bounds of an outlier, so the answer is mostly an educated guess based on the context of the plot.**
    + **I think it's important to note that the distribution of ESR < 20 has less variance than ESR > 20. Globulin is fairly evenly distributed with a few observations having higher levels in the blood, but the points are located too far away from the main body of the data. **

<!--
Discussion questions:
Lisst any assumptions you made that werenâ€™t explicitly given in the problem.  
Did you get the results you expected? 
Did you run into any complications?  
Did you agree with your collaborators about the results?
Tell me anything you think is important here.
-->

2. (Multiple Regression) Continuing from the lecture on the \textbf{hubble} data from \textbf{gamair} library;

    a) Fit a quadratic regression model, i.e.,a model of the form
$$\text{Model 2:   } velocity = \beta_1 \times distance + \beta_2 \times distance^2 +\epsilon$$
    ```{r, echo=FALSE}
    #import library
    library(gamair)
    data(hubble)
    hubble.df = hubble
    
    #calculate the distance squared and add to hubble.df
    model2 = lm(y ~ x + I(x^2), data = hubble.df)
    model3 = lm(y ~ x, data = hubble.df)
    
    #organize the values to make base r graph look better
    newdat = data.frame(x = seq(min(hubble.df$x), max(hubble.df$x), length.out = 100))
    newdat$pred = predict(model2, newdata = newdat)
    ```

    b) Plot the fitted curve from Model 2 on the scatterplot of the data
    
    c) Add the simple linear regression fit (fitted in class) on this plot - use different color and line type to differentiate the two and add a legend to your plot. 
    
    ```{r, echo=FALSE}
    #create plots in base r and ggplot for the quadratic regression and simple linear regression
    plot(x = hubble.df$x, hubble$y)
    abline(model3, col = 'red')
    with(newdat, lines(x = x, y = pred))
    legend("topleft", legend=c("simple", "quadratic"),
           col=c("red", "black"), lty=1, cex=0.8)
    
    ggplot(hubble.df, aes(x=hubble.df$x, y=hubble.df$y)) + geom_point() + 
      geom_line(data = fortify(model2), aes(x=hubble.df$x, y = .fitted, colour='blue'), show.legend = TRUE) + 
      geom_line(data = fortify(model3), aes(x=hubble.df$x, y = .fitted, colour='red'), show.legend = TRUE) +
      scale_color_discrete(name = "Model", labels = c("quad", "simple"))
    
    ```
    
    d) Which model do you consider most sensible considering the nature of the data - looking at the plot? 
    
    **Looking at the plots, it seems as though both fit the data fairly well. I think that the quadratic model fits slightly better on the lower end but worse in the middle and the top end. Considering the plot, the linear model seems more sensible (but this may be incorrect without having looked at MSE yet).**
    
    e) Which model is better? - provide a statistic to support you claim.
    
    **The simple linear regression seems to perform slightly better than the quadratic regression model.**
    ```{r, echo=FALSE}
    #calculate mean squated error of the different models
    MSE1 = mean((predict(model2, newdat, type = 'response') - hubble.df$x)^2)
    cat('Quadratic regression MSE =     ', MSE1, '\n')
    MSE2 = mean((predict(model3, newdat, type = 'response') - hubble.df$x)^2)
    cat('Simple linear regression MSE = ', MSE2, '\n')
    
    ```
    
    Note: The quadratic model here is still regarded as a ``linear regression" model since the term ``linear" relates to the parameters of the model and not to the powers of the explanatory variables. 

3. The \textbf{leuk} data from package \textbf{MASS} shows the survival times from diagnosis of patients suffering from leukemia and the values of two explanatory variables, the white blood cell count (wbc) and the presence or absence of a morphological characteristic of the white blood cells (ag). 

    a) Define a binary outcome variable according to whether or not patients lived for at least 24 weeks after diagnosis. Call it \textit{surv24}. 

    ```{r, echo=FALSE}
    #import library and create surv24 vector
    library(MASS)
    data(leuk, package = 'MASS')
    leuk.df = leuk
    
    surv24 = ifelse(leuk.df$time >= 24, 1, 0)
    ```
    
    b) Fit a logistic regression model to the data with \textit{surv24} as response. It is advisable to transform the very large white blood counts to avoid regression coefficients very close to 0 (and odds ratio close to 1). You may use log transformation.
    
    ```{r, echo=FALSE}
    #calculate log of wbc and create a logistic regression without interaction
    leuk.df$wbcLog = log10(leuk.df$wbc)
    
    model.logistic = glm(surv24~wbcLog + ag, data = leuk.df, family=binomial())
    ```
    
    c) Construct some graphics useful in the interpretation of the final model you fit. 
    
    ```{r, echo=FALSE}
    #following plot from example in GLM_CH7.R to create plot using base R
    p = leuk.df$ag == 'present'
    surviving.pred = predict(model.logistic, type = 'response')
    plot(leuk.df$wbcLog, surv24, col=leuk.df$ag, ylab="Probability of surviving", xlab="log10(wbc)", ylim=c(0,1))
    lines(leuk.df$wbcLog[!p], surviving.pred[!p], lty=1)
    lines(leuk.df$wbcLog[p], surviving.pred[p], lty=2)
    legend('topright', legend=c("absent", "present"), lty=1:2, bty='n')
    #create same plot using ggplot
    leuk.df = cbind(leuk.df, preds = surviving.pred)
    ggplot(leuk.df, aes(x=leuk.df$wbcLog, y=surv24, colour=ag))+ geom_point()+
      geom_line(data=leuk.df[leuk.df$ag =='present',], aes(x=wbcLog, y=preds), colour='blue')+
      geom_line(data=leuk.df[leuk.df$ag == 'absent',], aes(x=wbcLog, y=preds), colour='red')+
      scale_color_discrete(name = "Results")
    #scatter plot between log wbc and time survived after diagnosis
    ggplot(leuk.df, aes(x=time, y=wbcLog, colour=ag))+
      geom_point()
    ```
    
    d) Fit a model with an interaction term between the two predictors. Which model fits the data better? Justify your answer.
    
    ```{r, echo=FALSE}
    #create logistic regression with interaction and compare MSE to model without interaction
    model.logistic2 = glm(surv24~wbcLog + ag + wbcLog * ag, data=leuk.df, family=binomial())
    MSE3 = mean((predict(model.logistic, leuk.df, type='response') - surv24)^2)
    cat('no interaction model MSE:', MSE3, '\n')
    MSE4 = mean((predict(model.logistic2, leuk.df, type='response') - surv24)^2)
    cat('interaction model MSE:   ', MSE4, '\n')
    ```
    
**The interaction model has a lower MSE which is indicative of a better fit to the observations than the non-interaction model does. This means that the information together are better predictors of whether the patient will or will not survive past 24 weeks after diagnosis.**
    
**Dicussion: This exercise was difficult only being of the plotting. Trying to get the plots to look right was really difficult. The example given in the GLM_CH7.R for plotting the agree and disagree was not directly applicable here which caused some difficulties. The creation of the model wasn't awfully difficult to perform.**
    
+ **Looking at the graphs, up until about 3.5 to 4 log10 of the WBC do the data points begin to become mixed as to whether or not the patient lived longer than 24 weeks after diagnosis. However, the 2 groups of cellular morphology (absent and present) are rather distinct in the values that are predicted which can be viewed in the base R and ggplot graphs. The probability of surviving with cellular morphology present starts to decrease faster as the log10 of WBC increases whereas the probability of surviving past 24 weeks with the absence of cellular morphology levels off as log10 of WBC increases. This also shows the importance of cellular morphology in predicting whether the patient will survive past 24 weeks. The final scatter plot shows more clearly that those with ag present will survive longer than those without it.**

4. Load the \textbf{Default} dataset from \textbf{ISLR} library. The dataset contains information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt. It is a four-dimensional dataset with 10000 observations. The question of interest is to predict individuals who will default . We want to examine how each predictor variable is related to the response (default). Do the following on this dataset 

    a) Perform descriptive analysis on the dataset to have an insight. Use summaries and appropriate exploratory graphics to answer the question of interest.
    
    ```{r, echo=FALSE}
    #import library
    library(ISLR)
    data("Default")
    default.df = Default
    ```

**Summary Statistics**

    ```{r, echo=FALSE}
    #summary statistics split by default column
    summary(default.df[default.df$default=='Yes',])
    summary(default.df[default.df$default=='No',])
    ```
    
**Boxplots to show the difference in the balance and income of those that will default**
    
```{r, echo=FALSE}
#create plots compaing income and balance to whether they defaulted
attach(default.df)
par(mfrow = c(1,2))
boxplot(balance~default)
boxplot(income~default)
ggplot(default.df, aes(x = default, y = balance))+ geom_boxplot()
ggplot(default.df, aes(x = default, y = income))+ geom_boxplot()
detach(default.df)
```
    
    b) Use R to build a logistic regression model. 
    
    ```{r, echo=FALSE}
    #create vector for whether or not they defaulted and create interaction model and non-interaction model
    defs = ifelse(default.df$default == 'Yes', 1, 0)
    defs.model = glm(defs~student + balance + income + student*balance + student*income + balance*income, data=default.df, family=binomial())
    defs.model2 = glm(defs~student + balance + income, data=default.df, family=binomial())
    ```
    
    c) Discuss your result. Which predictor variables were important? Are there interactions?
    
    ```{r, echo=FALSE}
    #summary of the interaction model
    summary(defs.model)
    ```
    
**Including the interaction term coefficients in the model show that the only significant term is the balance. None of the interaction coefficients are significantly different than zero and therefore are not associated (together) in whether or not they will default.**
    
    ```{r, echo=FALSE}
    #summary of the non-interaction model
    summary(defs.model2)
    ```
    
**Removing the interaction terms in the model shows that balance is still a significant term. Whether or not the individual is a student is also significant in predicting whether the individual is going to default or not which is interesting. Balance is actually more significant in the model without interaction terms than it is with them.**
    
    d) How good is your model? Assess the performance of the logistic regression classifier. What is the error rate? 
    
```{r, echo=FALSE}
#comparison of the MSE between the interaction model and the model without interaction
MSE5 = mean((predict(defs.model, default.df, type = 'response') - defs)^2)
cat('Logistic regression with interaction terms MSE =    ', MSE5, '\n')
MSE6 = mean((predict(defs.model2, default.df, type = 'response') - defs)^2)
cat('Logistic regression without interaction terms MSE = ', MSE6, '\n')
```

**The MSE of the model with interaction coefficients is slightly lower than the MSE from the model without interactions. THey are almost identical, down to the 5th decimal place after which they are different. **

```{r, echo=FALSE}
#Confusion matrices to determine the accuracy of the logistic regression models
defs1 = predict(defs.model, default.df, type = 'response')
defs1 = ifelse(defs1 >= 0.5, 1, 0)
table1 = table(defs1, defs)
print('Accuracy of LRM with interaction: ')
sum(diag(table1))/sum(table1)

defs2 = predict(defs.model2, default.df, type = 'response')
defs2 = ifelse(defs2 >= 0.5, 1, 0)
table2 = table(defs2, defs)
print('Accuracy of LRM without interaction: ')
sum(diag(table2))/sum(table2)
```
    
**Looking at confusion matrices of the predictions with the true results, and the accuracy of both models is greater than 97%.  The model with interaction has a slightly lower accuracy (97.31% v 97.32%) but they are basically the same here too. It is interesting to note that the MSE of the model with interaction was lower than without and the accuracy was slightly better.**

5. Go through Section 7.3.1 of the Handbook. Run all the codes (additional exploration of data is allowed) and write your own version of explanation and interpretation.

Logistic Regression and Generalized Linear Models

```{r, echo=FALSE}
#creates density plot by frequency of ESR based on protein level
data("plasma", package="HSAUR3")
layout(matrix(1:2, ncol=2))
cdplot(ESR ~ fibrinogen, data = plasma)
cdplot(ESR ~ globulin, data = plasma)
```

**These are the distributions of fibinogen and globulin based on the ESR value being lower or higher than 20. The ESR changes greatly as fibinogen increases. The ESR distibution changes a decent amount as globulin increases, but the change is less than that of fibrinogen.**
```{r, echo=FALSE}
#logistic regression model for ESR based on fibrinogen
plasma_glm_1 = glm(ESR ~ fibrinogen, data = plasma, family=binomial())
```

```{r, echo=FALSE}
#fibrinogen coefficient 95% confidence interval
confint(plasma_glm_1, parm = "fibrinogen")
``` 

```{r, echo=FALSE}
#logistic regression summary
summary(plasma_glm_1)
```

**Creating a logistic regression model for ESR using fibrinogen shows that as fibrinogen increases, ESR also increases. The fibrinogen coefficient is significant with a p-value of 0.0425. The estimated coefficient is 1.827 and has a 95% confidence interval between 0.339 and 3.998.**

```{r, echo=FALSE}
#the edds of fibrinogen coefficnet
exp(coef(plasma_glm_1)["fibrinogen"])
```

```{r, echo=FALSE}
#conficent interval odds of fibrinogen
exp(confint(plasma_glm_1, parm = "fibrinogen"))
```

**These values are the exponents of the coefficient and confidence interval which are the odds themselves? This is the odds that the response variable, ESR, increases by 1 when fibinogen increases by 1 conditional on all other variables remaining constant (which are none in this case). The confidence interval is exceptionally large, possibly due to only 6 of the 32 data points having an ESR greater than 20.**

```{r, echo=FALSE}
#logistic regression for ESR using both fibrinogen and globulin as input variables
plasma_glm_2 <- glm(ESR ~ fibrinogen + globulin, data = plasma, family = binomial())
```

**Creating a logistic regression model to predict ESR based in independent variables fibrinogen and globulin, without looking at any interactions.**

```{r, echo=FALSE}
#summary of fibrinogen/globulin model
summary(plasma_glm_2)
```

**From the summary of the logistic regression created using both fibrinogen and globulin as input variables, shows that the fibrinogen term coefficient is significant just like the previous model using only fibrinogen as an input variable. The coefficient for globulin isn't significantly different than 0. **

```{r, echo=FALSE}
#chi-squared test between the model without globulin and with globulin
anova(plasma_glm_1, plasma_glm_2, test = "Chisq")
```

**Comparing the two different models, we can see that addinng the globulin term doesn't make the model significantly different from the model without it, and therefore can see that globulin doesn't really contribute to the ESR value.**

```{r, echo=FALSE}
#predictions from the model with globulin
prob <- predict(plasma_glm_2, type = "response")
```

```{r, echo=FALSE}
#creating bubble plot of fibrinogen and globulin, with probability of the ESR being greater than 20 as the bubble size
plot(globulin ~ fibrinogen, data = plasma, xlim = c(2, 6), ylim = c(25, 55), pch = ".")
symbols(plasma$fibrinogen, plasma$globulin, circles = prob, add = TRUE)
```

**As seen with the chi-squared test and the second regression model, globulin doesn't contribute greatly to the ESR. Fibrinogen contributes a lot more to the ESR and the plot shows that as fibrinogen increases, the probability of ESR being greater than 20 increases drastically (larger circle indicates a greater probability of having an ESR greater than 20). **

```{r, echo=FALSE}

```

```{r, echo=FALSE}

```

```{r, echo=FALSE}

```

```{r, echo=FALSE}

```

```{r, echo=FALSE}

```



*Resources Used*:

+ StackOverflow
+ community.rstudio.com
+ stats.idre.ucla.edu